{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP90049 Introduction to Machine Learning, 2020 Semester 1\n",
    "-----\n",
    "## Project 1: Understanding Student Success with Naive Bayes\n",
    "-----\n",
    "###### Student Name: Benjamin De Worsop\n",
    "###### Python version: Python 3.6.8\n",
    "###### Submission deadline: 11am, Wed 22 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Project 1 submission. \n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 is found below\n",
    "## 1) Part A\n",
    "##### Explain the ‘naive’ assumption underlying Naive Bayes. (1) Why is it necessary? (2) Why can it be problematic? Link your discussion to the features of the students data set. [no programming required]\n",
    "\n",
    "The Naive assumption in the Naive Bayes model is the assumption that each of the classes are all independent of each other. This is required to simply modelling probabilities of events. It allows you to compute the probabilities of combinations of events that haven't been seen before whilst still applying the logic behind Bayes rule. This can be seen in the example:\n",
    "\n",
    "p(A+ | internet, absences) = p(internet, abences|A+)*p(A+)/p(internet, absences)\n",
    "\n",
    "It is very difficult to find these probabilities. Instead, we can assume independence to get this equation:\n",
    "\n",
    "p(A+ | internet, abences) = \n",
    "p(internet|A+)*p(abences|A+)*p(A+)/p(internet)*p(abences)\n",
    "\n",
    "This is much easier to solve for because the variables are easier to measure as there is usually more data available on the occurrence of two overlapping variables (p(internet | A+)) than many overlapping variables. So, To be able to measure the probabilities of instances with previously unseen class combinations and to use more samples to guide probabilities, this assumption is necessary.\n",
    "\n",
    "Unfortunately, predicting power is sacrificed when this assumption is made. The labelled probabilities of events are weaker approximations of their real probabilities. This is especially so when the assumption is less correct (the features are not very independent). In our class example, there are many values that we would assume are correlated like parents' jobs/education, extra paid class attendance and address which are all linked to wealth. Assuming these factors are probabilistically independent may be flawed, thus creating a more inaccurate result.\n",
    "\n",
    "\n",
    "## 1) Part B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"student.csv\"\n",
    "\n",
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data\n",
    "\n",
    "rawData = load_data(path)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This function should split a data set into a training set and hold-out test set\n",
    "def split_data(rawData):\n",
    "    return train_test_split(rawData, test_size=0.1)\n",
    "\n",
    "trainData, testData = split_data(rawData)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "featureFrames = None\n",
    "GRADES = [\"A+\", \"A\",\"B\",\"C\",\"D\",\"F\"]\n",
    "\n",
    "\n",
    "def make_grade_struct():\n",
    "    #Create the grades df manually inserting them as column/row names\n",
    "    featureFrames[len(list(rawData))-1] = pd.DataFrame(columns=GRADES, index=GRADES)\n",
    "\n",
    "def make_data_structs():\n",
    "    global featureFrames\n",
    "    featureFrames = []\n",
    "    paramNames = []\n",
    "\n",
    "    #For each feature\n",
    "    for feature in list(rawData):\n",
    "        \n",
    "        #Find all the names of the paramaters by iterating through all data\n",
    "        for j in range(len(rawData)):\n",
    "            if str(rawData[feature][j]) not in paramNames:\n",
    "                paramNames.append(str(rawData[feature][j]))        \n",
    "        \n",
    "        #Make a dataframe to store frequency information with format:     \n",
    "        #     A+    A    B    C    D    F\n",
    "        # T  NaN  NaN  NaN  NaN  NaN  NaN\n",
    "        # A  NaN  NaN  NaN  NaN  NaN  NaN\n",
    "        \n",
    "        df = pd.DataFrame(columns=GRADES, index=paramNames)\n",
    "        featureFrames.append(df.copy())\n",
    "\n",
    "        #Reset paramNames \n",
    "        paramNames = []\n",
    "        \n",
    "    #In case we don't see all the grade values, do this manually \n",
    "    make_grade_struct()    \n",
    "    \n",
    "def count_param_freq(data):\n",
    "    #For every feature\n",
    "    for i, feature in enumerate(list(data)):\n",
    "        \n",
    "        #Go and count the number of times each paramater resulted in what grade\n",
    "        for j in range(len(data)):\n",
    "\n",
    "            #Define the parameter feature, grade, and position in data struct\n",
    "            param = str(data[feature].iloc[j])\n",
    "            grade = data[\"Grade\"].iloc[j]\n",
    "            freqCount = featureFrames[i][grade][param]\n",
    "        \n",
    "            #Count the number of times that this parameter is seen\n",
    "            if pd.isnull(freqCount):\n",
    "                featureFrames[i][grade][param] = 1\n",
    "            else:\n",
    "                featureFrames[i][grade][param]+=1\n",
    "                 \n",
    "\n",
    "# This function should build a supervised NB model\n",
    "def train(data):\n",
    "    make_data_structs()\n",
    "    count_param_freq(data)\n",
    "    return\n",
    "\n",
    "\n",
    "train(trainData)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance number 0 is predicted to be = D, actual grade = F\n",
      "instance number 1 is predicted to be = C, actual grade = B\n",
      "instance number 2 is predicted to be = D, actual grade = C\n",
      "instance number 3 is predicted to be = C, actual grade = D\n",
      "instance number 4 is predicted to be = C, actual grade = B\n",
      "instance number 5 is predicted to be = F, actual grade = F\n",
      "instance number 6 is predicted to be = F, actual grade = F\n",
      "instance number 7 is predicted to be = C, actual grade = C\n",
      "instance number 8 is predicted to be = B, actual grade = D\n",
      "instance number 9 is predicted to be = C, actual grade = B\n",
      "instance number 10 is predicted to be = C, actual grade = A\n",
      "instance number 11 is predicted to be = A, actual grade = C\n",
      "instance number 12 is predicted to be = A, actual grade = A+\n",
      "instance number 13 is predicted to be = D, actual grade = D\n",
      "instance number 14 is predicted to be = B, actual grade = B\n",
      "instance number 15 is predicted to be = F, actual grade = F\n",
      "instance number 16 is predicted to be = A, actual grade = A\n",
      "instance number 17 is predicted to be = D, actual grade = D\n",
      "instance number 18 is predicted to be = D, actual grade = D\n",
      "instance number 19 is predicted to be = C, actual grade = A\n",
      "instance number 20 is predicted to be = D, actual grade = D\n",
      "instance number 21 is predicted to be = D, actual grade = C\n",
      "instance number 22 is predicted to be = C, actual grade = D\n",
      "instance number 23 is predicted to be = B, actual grade = A\n",
      "instance number 24 is predicted to be = A, actual grade = B\n",
      "instance number 25 is predicted to be = F, actual grade = D\n",
      "instance number 26 is predicted to be = C, actual grade = B\n",
      "instance number 27 is predicted to be = A, actual grade = C\n",
      "instance number 28 is predicted to be = C, actual grade = B\n",
      "instance number 29 is predicted to be = D, actual grade = F\n",
      "instance number 30 is predicted to be = D, actual grade = A\n",
      "instance number 31 is predicted to be = D, actual grade = C\n",
      "instance number 32 is predicted to be = D, actual grade = D\n",
      "instance number 33 is predicted to be = C, actual grade = B\n",
      "instance number 34 is predicted to be = A, actual grade = A\n",
      "instance number 35 is predicted to be = A, actual grade = C\n",
      "instance number 36 is predicted to be = D, actual grade = D\n",
      "instance number 37 is predicted to be = C, actual grade = C\n",
      "instance number 38 is predicted to be = C, actual grade = D\n",
      "instance number 39 is predicted to be = F, actual grade = F\n",
      "instance number 40 is predicted to be = D, actual grade = F\n",
      "instance number 41 is predicted to be = B, actual grade = B\n",
      "instance number 42 is predicted to be = B, actual grade = A\n",
      "instance number 43 is predicted to be = D, actual grade = D\n",
      "instance number 44 is predicted to be = A, actual grade = C\n",
      "instance number 45 is predicted to be = D, actual grade = F\n",
      "instance number 46 is predicted to be = D, actual grade = D\n",
      "instance number 47 is predicted to be = F, actual grade = D\n",
      "instance number 48 is predicted to be = B, actual grade = D\n",
      "instance number 49 is predicted to be = C, actual grade = C\n",
      "instance number 50 is predicted to be = A, actual grade = C\n",
      "instance number 51 is predicted to be = A, actual grade = A\n",
      "instance number 52 is predicted to be = D, actual grade = A\n",
      "instance number 53 is predicted to be = A+, actual grade = B\n",
      "instance number 54 is predicted to be = B, actual grade = B\n",
      "instance number 55 is predicted to be = B, actual grade = A\n",
      "instance number 56 is predicted to be = A, actual grade = A\n",
      "instance number 57 is predicted to be = A, actual grade = C\n",
      "instance number 58 is predicted to be = B, actual grade = F\n",
      "instance number 59 is predicted to be = A+, actual grade = A+\n",
      "instance number 60 is predicted to be = D, actual grade = C\n",
      "instance number 61 is predicted to be = C, actual grade = D\n",
      "instance number 62 is predicted to be = C, actual grade = B\n",
      "instance number 63 is predicted to be = C, actual grade = D\n",
      "instance number 64 is predicted to be = D, actual grade = B\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import random\n",
    "\n",
    "eps = None\n",
    "totTrainingRows = None\n",
    "\n",
    "\n",
    "def find_probability(testGrade, testInst):\n",
    "\n",
    "    #P(label|params) = const * p(param1|label) * p(param2|label) * ... * p(label)\n",
    "    probability = 1\n",
    "    \n",
    "    #For every parameter, find the probability and update main prob\n",
    "    #for every feature name\n",
    "    for i, label in enumerate(rawData):\n",
    "        #Don't use the \"Grade\" column for the instance for predictions\n",
    "        #otherwise you're cheating!\n",
    "        if i == len(list(rawData))-1:\n",
    "            break\n",
    "            \n",
    "\n",
    "        # p(param1|label) = numberParam1|label / numberOfLabel\n",
    "        #Get the actual paramater we're finding the probability of\n",
    "        testParam = str(testInst[label])\n",
    "        \n",
    "        #Number of times this prameter was counted in training given the label\n",
    "        try:\n",
    "            #Try find the label, if you can't find it, or the value is null, probability *= eps\n",
    "            paramFreq = featureFrames[i][testGrade][testParam]       \n",
    "            if pd.isnull(paramFreq):\n",
    "                probability*=eps\n",
    "                continue\n",
    "            #The number of times this grade has been counted in training\n",
    "            gradeFreq = featureFrames[len(featureFrames)-1][testGrade][testGrade]            \n",
    "       \n",
    "            #Update probability\n",
    "            probability *= (paramFreq/gradeFreq)\n",
    "            \n",
    "        except:\n",
    "            probability*=eps\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        probability *= gradeFreq/totTrainingRows\n",
    "    except:\n",
    "        probability = eps\n",
    "        \n",
    "\n",
    "    return probability\n",
    "\n",
    "def predict_grade(instance):\n",
    "    labelProbability = {\"A+\":0,\"A\":0,\"B\":0,\"C\":0,\"D\":0,\"F\":0}\n",
    "    \n",
    "    #For every potential label (A+, A, B, C, D, F), find P(label|params)\n",
    "    for label in labelProbability.keys():\n",
    "        \n",
    "        labelProbability[label] = find_probability(label, instance)  \n",
    "        \n",
    "    return max(labelProbability.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "\n",
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(data):\n",
    "    global eps\n",
    "    global totTrainingRows\n",
    "    eps = 1/len(data)\n",
    "    totTrainingRows = len(data)    \n",
    "    \n",
    "    for i, inst in enumerate(data.iterrows()):\n",
    "        print(\"instance number {} is predicted to be = {}, actual grade = {}\".format(i, predict_grade(inst[1]), inst[1][\"Grade\"]))\n",
    "    \n",
    "\n",
    "\n",
    "predict(testData)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "accuracy is 0.35384615384615387. (23 successes and 42 fails)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "accuracy = None\n",
    "\n",
    "\n",
    "# This function should evaluate a set of predictions in terms of accuracy\n",
    "def evaluate(data):\n",
    "    count = 0\n",
    "    global accuracy\n",
    "    accuracy = {\"success\":0, \"fail\":0}\n",
    "    print(\"evaluating\")\n",
    "    \n",
    "    #Go through every row and count if the prediction was a success or fail\n",
    "    for inst in data.iterrows():\n",
    "        if(predict_grade(inst[1]) == inst[1][\"Grade\"]):\n",
    "            accuracy[\"success\"] += 1\n",
    "        else:\n",
    "            accuracy[\"fail\"] += 1\n",
    "    \n",
    "    s = accuracy[\"success\"]\n",
    "    f = accuracy[\"fail\"]\n",
    "    print(\"accuracy is {}. ({} successes and {} fails)\".format((s/(s+f)), s, f))\n",
    "            \n",
    "    return (s/(s+f))\n",
    "\n",
    "evaluate(testData)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Part C\n",
    "\n",
    "#### What accuracy does your classiﬁer achieve? Manually inspect a few instances for which your classiﬁer made correct predictions, and some for which it predicted incorrectly, and discuss any patterns you can ﬁnd.\n",
    "\n",
    "From inspection, the accuracy of this model is around 30% for an 80-20 train-test data split. This is approximately twice as good as randomly guessing (\\~16.6%) and about as good as guessing the most common class (\\~30%). Observing the classifier output, it seems that this Naive Bayes model produces the better results more common classes within the dataset (Grade = C and D), and poorly with less common classes (Grade = A+). This may be due to instances with less common classes having less opportunity to have it's defining characteristics become more salient in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "### A Closer Look at Evaluation\n",
    "\n",
    "#### - A) You learnt in the lectures that precision, recall and f-1 measure can provide a more holistic and realistic picture of the classifier performance. (i) Explain the intuition behind accuracy, precision, recall, and F1-measure, (ii) contrast their utility, and (iii) discuss the difference between micro and macro averaging in the context of the data set. [no programming required]\n",
    "#### - B) Compute precision, recall and f-1 measure of your model’s predictions on the test data set (1) separately for each class, and (2) as a single number using macro-averaging. Compare the results against your accuracy scores from Question 1. In the context of the student dataset, and your response to question 2a analyze the additional knowledge you gained about your classifier performance.\n",
    "\n",
    "\n",
    "\n",
    "## 2) Part a\n",
    "\n",
    "##### Intuition\n",
    "Accuracy - This is the proportion of guesses we got right compared to all guesses. I.e. How many times was our prediction accurate? More formally:\n",
    "\n",
    "(True positives + True Negatives)/(True or Flase positives or negatives)\n",
    "\n",
    "\n",
    "Precision - For a class that we care to measure (maybe A+ students in this example), how often is our model correct. If we care about one label more than the others, we want to see how accurately we can predict that class. I.e. When our model detects an interesting class, is it right? More formally:\n",
    "\n",
    "(True positives)/(True or False positives)\n",
    "\n",
    "\n",
    "Recall - How good is our model at detecting our desired class. More formally:\n",
    "\n",
    "(True positives)/(True positives + False Negatives)\n",
    "\n",
    "\n",
    "F1 measure - A combined measure of precision and recall using the harmonic mean. This is important because precision and recall are usually inversely correlated (models with higher recall have lower precisino and vice versa). It shows how good your classifier at balancing precision and recall. It is defined:\n",
    "\n",
    "(2 * Precision*Recall)/(Precision + Recall)\n",
    "\n",
    "\n",
    "##### Compare/Contrast:\n",
    "The value of precision and recall are based on your desire for type 1 vs type 2 error.\n",
    "\n",
    "If you are a doctor and your model is determining if a patient has cancer, you don't want it to tell patients who have cancer, that they dont have cancer (false negative). Better safe than sorry. This is where you don't mind false positives, so you can have a low precision, but you need a high recall. \n",
    "\n",
    "This is opposite if you don't care about letting false negatives pass, but want to make sure those that you classify as part of an interesting class is important. An example of this is an employer who wants close to 100% of the candidates they interview to be high quality, but doesn't mind rejecting some potentially great candidates to make this happen.\n",
    "\n",
    "F1 is important when you want a balance of these 2, and want to overall maximise your performance without having preference for type 1 or type 2 error.\n",
    "\n",
    "\n",
    "##### Micro vs macro averaging\n",
    "Macro averaging uses the precision and recall scores from each class and takes the average to produce one single score.\n",
    "\n",
    "Micro averaging first aggregates all of the TP, FP and FN results from all the classes, and then adds, and divides these results by eachother according to the precision or recall equation respectively. E.g.\n",
    "\n",
    "micro_precision = sum_TP/(sum_TP + sum_FP)\n",
    "\n",
    "\n",
    "## 2) Part b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the label A+, precision = 1.000, recall = 0.500, f-1 = 0.667\n",
      "For the label A, precision = 0.333, recall = 0.364, f-1 = 0.348\n",
      "For the label B, precision = 0.333, recall = 0.231, f-1 = 0.273\n",
      "For the label C, precision = 0.176, recall = 0.231, f-1 = 0.200\n",
      "For the label D, precision = 0.381, recall = 0.471, f-1 = 0.421\n",
      "For the label F, precision = 0.600, recall = 0.333, f-1 = 0.429\n",
      "macro averaging scores: precision = 0.471, recall = 0.355, f1 = 0.389,\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#First evaluate all the predicitons into TP, TN, FN and FP for each class\n",
    "\n",
    "#Setup dataframes for storing the TP, TN, FN and FP results\n",
    "labelAccuracy = {\"A+\":None,\"A\":None,\"B\":None,\"C\":None,\"D\":None,\"F\":None}\n",
    "\n",
    "#Initialises dataframe\n",
    "for label in labelAccuracy.keys():\n",
    "    labelAccuracy[label] = pd.DataFrame(0, index=[\"positive\", \"negative\"], columns=[\"true\", \"false\"]) \n",
    "\n",
    "#Safely adds one to the right column of dataframe\n",
    "def add_one(label, truFal, posNeg):\n",
    "    if pd.isnull(labelAccuracy[label][truFal][posNeg]):\n",
    "        labelAccuracy[label][truFal][posNeg] = 1\n",
    "    else:\n",
    "        labelAccuracy[label][truFal][posNeg] += 1\n",
    "\n",
    "#Evaluate and count a particular type of error       \n",
    "def count_error_type(label, predictedGrade, actualGrade):\n",
    "    if(predictedGrade == actualGrade and predictedGrade == label):\n",
    "        add_one(label, \"true\", \"positive\")\n",
    "    elif(label != predictedGrade and label != actualGrade):\n",
    "        add_one(label, \"true\", \"negative\")\n",
    "    elif(label == predictedGrade and label != actualGrade):\n",
    "        add_one(label, \"false\", \"positive\")\n",
    "    elif(label != predictedGrade and label == actualGrade):\n",
    "        add_one(label, \"false\", \"negative\")\n",
    "    \n",
    "# This function should evaluate a set of predictions in terms of accuracy\n",
    "def evaluate_F1(data):\n",
    "    #For each instance\n",
    "    for inst in data.iterrows():\n",
    "        \n",
    "        predictedGrade = predict_grade(inst[1])\n",
    "        actualGrade = inst[1][\"Grade\"]\n",
    "        \n",
    "        #Go through all the labels and find if it evaluated a TP, TN, FN or FP\n",
    "        for label in labelAccuracy.keys():\n",
    "            #If interesting key is correct, label as correct\n",
    "            count_error_type(label, predictedGrade, actualGrade)\n",
    "    return\n",
    "\n",
    "eps = 1e-30\n",
    "\n",
    "#Returns 0 if illegal division\n",
    "def safe_divide(num, denom):\n",
    "    if (denom < eps):\n",
    "        return 0\n",
    "    else:\n",
    "        return num/denom\n",
    "    \n",
    "#Finds the average of a value found in a particular list location\n",
    "def ave(lst, index): \n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for indList in lst:\n",
    "        sum += indList[index]\n",
    "        count += 1\n",
    "    return sum/count        \n",
    "\n",
    "def print_func():\n",
    "    mac_ave = []\n",
    "    \n",
    "    #Go through every grade (a key of the labelAccuracy dict)\n",
    "    # and calculate important values\n",
    "    for key in labelAccuracy.keys():\n",
    "        theKey = key\n",
    "        la = labelAccuracy\n",
    "        \n",
    "        precision = safe_divide(la[key][\"true\"][\"positive\"],(la[key][\"true\"][\"positive\"] + la[key][\"false\"][\"positive\"]))\n",
    "        recall = safe_divide(la[key][\"true\"][\"positive\"],(la[key][\"true\"][\"positive\"] + la[key][\"false\"][\"negative\"]))\n",
    "        f1 = safe_divide((2*precision*recall),(precision + recall))\n",
    "        mac_ave.append([key, precision, recall, f1].copy())\n",
    "      \n",
    "        print(\"For the label {}, precision = {:.3f}, recall = {:.3f}, f-1 = {:.3f}\".format(key, precision, recall, f1))\n",
    "    print(\"macro averaging scores: precision = {:.3f}, recall = {:.3f}, f1 = {:.3f},\".format(ave(mac_ave, 1),ave(mac_ave,2),ave(mac_ave,3)))\n",
    "        \n",
    " \n",
    "\n",
    "evaluate_F1(testData)\n",
    "print_func()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Part b continued\n",
    "\n",
    "#### Compare the results against your accuracy scores from Question 1. In the context of the student dataset, and your response to question 2a analyze the additional knowledge you gained about your classiﬁer performance.\n",
    "\n",
    "\n",
    "The data gathered from part b supports the hypothesis created in Question 1 - that less common classes were predicted less accurately than more common classes around 20-30%. While the D and F classes can be predicted up to 40-50% of the time on a similar 80-20 data split, there are close-to or exactly 0 correct predictions of the A+ class depending on the random sampling. \n",
    "\n",
    "The model sometimes performs much better in recall for common classes like \"C\", \"D\" and \"F \". This suggests a high number of false negatives were made for these classes. The prevelance of these classes suggest that this conservative guessing may be due to overfitting of their class traits. If the model has a very clear idea of what the \"C\", \"D\" and \"F\" classes looks like, it may be conservative with its guesses. A more balanced data set may help improve the accuracy of this classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question 3: \n",
    "### Training Strategies \n",
    "\n",
    "#### There are other evaluation strategies, which tend to be preferred over the hold-out strategy you implemented in Question 1.\n",
    "\n",
    "#### - A) Select one such strategy, (i) describe how it works, and (ii) explain why it is preferable over hold-out evaluation. [no programming required]\n",
    "\n",
    "#### - B) Implement your chosen strategy from Question 3a, and report the accuracy score(s) of your classifier under this strategy. Compare your outcomes against your accuracy score in Question 1, and explain your observations in the context of your response to question 3a.\n",
    "\n",
    "\n",
    "\n",
    "## 3) Part a - Cross Validation\n",
    "\n",
    "\n",
    "Cross validation separates the dataset into n chunks and iteratively uses each chunk as testing data, with the rest as training data. E.g. if n=3, on iteration 1: chunk 1 = test data, chunk 2 and 3 are used as training. Then, on iteration 2: chunk 2 = test, and chunk 1 and 3 are used for training. This process is repeated till every chunk is used as a test.\n",
    "\n",
    "This is more effective as the hold-out strategy because it uses all of the data present in the set, unlike the holdout that uses one selection for testing and one selection for training. This makes the assessment more reflective of its overall performance using all the data.\n",
    "\n",
    "By design, each run uses lots of data for training. As n increases, the model can be trained with more data on each iteration, making iterations more accurate. Having the model be the most informed during training creates the best proxy of how it would perform in real life.\n",
    "\n",
    "Increasing the number of times the data is run gives a more accurate representation of the model's performance too. Outlier data doesn't affect the model as much because the final accuracy results aggregate all the previous results. The hold-out strategy is affected by this because it is only run once with possibly outlier-containing data.\n",
    "\n",
    "It is also repeatable - giving flexibility to researchers to better understand how their model could be improved. \n",
    "\n",
    "\n",
    "\n",
    "## 3) Part b - Implementation of Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1...\n",
      "evaluating\n",
      "accuracy is 0.3317972350230415. (72 successes and 145 fails)\n",
      "\n",
      "Iteration number 2...\n",
      "evaluating\n",
      "accuracy is 0.35023041474654376. (76 successes and 141 fails)\n",
      "\n",
      "Iteration number 3...\n",
      "evaluating\n",
      "accuracy is 0.3640552995391705. (79 successes and 138 fails)\n",
      "\n",
      "average value = 0.3486943164362519\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "chunkSize = int(len(rawData)/n)\n",
    "accuracyLog = []\n",
    "\n",
    "#For each chunk, spit the data into testing and training data\n",
    "for i in range(n):\n",
    "    startIndex = i*chunkSize\n",
    "    endIndex = startIndex + chunkSize\n",
    "    \n",
    "    #Deals with ugly divisors len(rawData)/chunkSize\n",
    "    if i == n-1:\n",
    "        endIndex = len(rawData)-1\n",
    "    \n",
    "    #Split the train and test data\n",
    "    tempTrainDF = rawData.drop(rawData.loc[startIndex:endIndex]\\\n",
    "                               .index, inplace=False)\n",
    "    tempTestDF = rawData.loc[startIndex:endIndex]\n",
    "\n",
    "    #Train and evaluate results\n",
    "    print(\"Iteration number {}...\".format(i+1))\n",
    "    train(tempTrainDF)\n",
    "    accuracyLog.append(evaluate(tempTestDF))\n",
    "    print()\n",
    "    \n",
    "\n",
    "print(\"average value = {}\".format((sum(accuracyLog)/len(accuracyLog))))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Part b - Continued\n",
    "\n",
    "#### Report the accuracy score(s) of your classifier under this strategy. Compare your outcomes against your accuracy score in Question 1, and explain your observations in the context of your response to question 3a.\n",
    "\n",
    "Unfortunately, the benefits of cross-validation don't seem to be apparent in this example. After experimenting with train-test separation values manually, an interesting guessing pattern emerged. The accuracy tends to be similar, even with different training-test splits.\n",
    "\n",
    "The model, trained with low amounts of data, guessed primarily \"D\" and \"F\", the most popular label. This produced a similar accuracy of around 30%. As the amount of training data increased, it began predicting other classes. Although, even though it improved accuracy of less common classes, it reduced performance in more common classes almost linearly. This led to the overall accuracy remaining about the same. So, there was a 28-37% accuracy with most statistically significant splits (at a train-test 99-1 split, the results are not reliable because the law of large numbers don't apply and the results are too volatile).\n",
    "\n",
    "This behaviour can largely be explained by the disproportionate spread of classes in the data. Ridding this bias would reduce the current model's higher accuracy at low training volumes due to the successful guess-the-most-popular-class strategy used. It may also make the differences between the classes more salient, creating more accurate classifiers when provided more training data. Other effects may be the low correlation between the classes and high-low performing student outcomes and/or high levels of dependence between the classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
